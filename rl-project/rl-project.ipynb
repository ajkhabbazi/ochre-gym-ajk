{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ade1e1b6-01c8-4bf0-8530-8807bb5026d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Properties file weather station (G0800310) may be different from weather file used: C:\\Users\\arashjkh\\AppData\\Local\\anaconda3\\envs\\ochre-gym-env\\lib\\site-packages\\ochre_gym\\buildings\\bldg0112631-up11\\USA_CO_Denver.Intl.AP.725650_TMY3.epw\n",
      "Cooling setpoint is within 1C of heating setpoint in schedule file. Separating setpoints by at least 1C.\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import ochre_gym\n",
    "\n",
    "# --- Environment configuration ---\n",
    "ENV_NAME = \"bldg0112631-up11\"\n",
    "\n",
    "OBS_KEYS = [\n",
    "    \"Temperature - Indoor (C)\",\n",
    "    \"Temperature - Outdoor (C)\",\n",
    "    \"Energy Price ($)\",\n",
    "    \"Hour of Day\",\n",
    "]\n",
    "\n",
    "env = ochre_gym.load(\n",
    "    ENV_NAME,\n",
    "    override_equipment_controls={\"HVAC Heating\": [\"Setpoint\"]},\n",
    "    vectorize_actions=True,\n",
    "    vectorize_observations=True,\n",
    "    override_ochre_observations_with_keys=OBS_KEYS,\n",
    "    start_time=\"2018-01-01 00:00:00\",\n",
    "    episode_duration=\"31 days\",      # <-- ADD THIS\n",
    "    time_res=\"00:30\",\n",
    "    lookahead=\"12:00\",\n",
    "\n",
    "    dr_type=\"TOU\",\n",
    "    dr_subfolder=\"basic-v0\",\n",
    "    tou_price_file=\"time_of_use_price.csv\",\n",
    "\n",
    "    thermal_comfort_band_low=20.0,\n",
    "    thermal_comfort_band_high=23.0,\n",
    "    thermal_comfort_unit_penalty=10.0,\n",
    "    reward_scale=1.0,\n",
    "    log_to_file=False,\n",
    "    log_to_console=False,\n",
    ")\n",
    "\n",
    "# --- Discrete heating setpoints ---\n",
    "SETPOINTS = np.round(np.arange(20.0, 23.0 + 0.001, 0.5), 1)  # [20.0, 20.5, ..., 23.0]\n",
    "A = len(SETPOINTS)\n",
    "\n",
    "\n",
    "class HeatingSetpointDiscrete(gym.ActionWrapper):\n",
    "    \"\"\"Map discrete action indices to continuous heating setpoints.\"\"\"\n",
    "\n",
    "    def __init__(self, env, setpoints: np.ndarray):\n",
    "        super().__init__(env)\n",
    "        self.setpoints = np.asarray(setpoints, dtype=np.float32)\n",
    "        self.action_space = gym.spaces.Discrete(self.setpoints.size)\n",
    "\n",
    "    def action(self, a_idx: int) -> np.ndarray:\n",
    "        return np.array([self.setpoints[a_idx]], dtype=np.float32)\n",
    "\n",
    "\n",
    "# Wrap environment so the agent chooses discrete setpoint indices\n",
    "env = HeatingSetpointDiscrete(env, SETPOINTS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7eced7f1-b914-41b6-b10f-5dd20248045c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bins per feature (tune these):\n",
    "Tin_bins  = np.arange(18.0, 26.5, 0.5)   # Defines bins for indoor temperature from 18.0°C to 26.5°C in 0.5°C increments.\n",
    "Tout_bins = np.arange(-20.0, 45.0, 5.0)  # Defines bins for outdoor temperature from -20°C to 45°C in 5°C increments.\n",
    "Price_bins = np.array([0.0, 0.1, 0.2, 0.4, 1.0, 5.0])  # Defines price tiers (in $/kWh) representing different electricity cost levels.\n",
    "Hour_bins  = np.arange(0, 24 + 1, 3)     # Divides the 24-hour day into 8 bins, each spanning 3 hours.\n",
    "\n",
    "# Combine all bin definitions into a single list for iteration.\n",
    "BIN_EDGES = [Tin_bins, Tout_bins, Price_bins, Hour_bins]\n",
    "\n",
    "\n",
    "def discretize(obs_vec):\n",
    "    \"\"\"\n",
    "    Converts a continuous observation vector into discrete bin indices \n",
    "    for use in a tabular Q-learning algorithm.\n",
    "\n",
    "    Each feature (indoor temp, outdoor temp, price, hour) is assigned \n",
    "    to the index of the bin it falls into.\n",
    "    \"\"\"\n",
    "\n",
    "    # Observation order must match the list defined in override_ochre_observations_with_keys\n",
    "    # Example: [\"Temperature - Indoor (C)\", \"Temperature - Outdoor (C)\", \"Energy Price ($)\", \"Hour of Day\"]\n",
    "    tin, tout, price, hour = obs_vec[0], obs_vec[1], obs_vec[2], obs_vec[3]\n",
    "\n",
    "    # Group the observation values for iteration\n",
    "    vals = [tin, tout, price, hour]\n",
    "\n",
    "    # Initialize list to store the bin index for each feature\n",
    "    idxs = []\n",
    "\n",
    "    # Loop over each value and its corresponding bin edges\n",
    "    for v, edges in zip(vals, BIN_EDGES):\n",
    "        # np.digitize returns a bin number starting from 1 up to len(edges)\n",
    "        # (right=False) means each bin includes its left edge and excludes the right edge: [a_i, a_{i+1})\n",
    "        i = int(np.digitize(v, edges, right=False))\n",
    "\n",
    "        # Clamp the index so it's always within valid bounds (0 to len(edges))\n",
    "        # This prevents errors when values fall outside the defined bin ranges.\n",
    "        i = max(0, min(i, len(edges)))\n",
    "\n",
    "        # Add the bin index to the list\n",
    "        idxs.append(i)\n",
    "\n",
    "    # Return a tuple of bin indices (e.g., (7, 9, 2, 5))\n",
    "    # Tuples are hashable, meaning they can be used as keys in a Q-table dictionary.\n",
    "    return tuple(idxs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "446488e5-4ff5-4be9-94c5-21879ce55742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 01 | steps=1488 | return=-5007.49\n",
      "Episode 02 | steps=1488 | return=-4998.18\n",
      "Episode 03 | steps=1488 | return=-4964.75\n",
      "Episode 04 | steps=1488 | return=-4871.23\n",
      "Episode 05 | steps=1488 | return=-4716.51\n",
      "Episode 06 | steps=1488 | return=-4881.49\n",
      "Episode 07 | steps=1488 | return=-4846.11\n",
      "Episode 08 | steps=1488 | return=-4592.22\n",
      "Episode 09 | steps=1488 | return=-4682.47\n",
      "Episode 10 | steps=1488 | return=-4596.97\n",
      "Episode 11 | steps=1488 | return=-4502.89\n",
      "Episode 12 | steps=1488 | return=-4601.72\n",
      "Episode 13 | steps=1488 | return=-4372.66\n",
      "Episode 14 | steps=1488 | return=-4355.98\n",
      "Episode 15 | steps=1488 | return=-4454.29\n",
      "Episode 16 | steps=1488 | return=-4392.37\n",
      "Episode 17 | steps=1488 | return=-4298.58\n",
      "Episode 18 | steps=1488 | return=-4348.79\n",
      "Episode 19 | steps=1488 | return=-4470.22\n",
      "Episode 20 | steps=1488 | return=-4239.93\n",
      "Episode 21 | steps=1488 | return=-4271.18\n",
      "Episode 22 | steps=1488 | return=-4277.12\n",
      "Episode 23 | steps=1488 | return=-4396.59\n",
      "Episode 24 | steps=1488 | return=-4372.86\n",
      "Episode 25 | steps=1488 | return=-4351.59\n",
      "Episode 26 | steps=1488 | return=-4295.19\n",
      "Episode 27 | steps=1488 | return=-4377.91\n",
      "Episode 28 | steps=1488 | return=-4356.66\n",
      "Episode 29 | steps=1488 | return=-4316.21\n",
      "Episode 30 | steps=1488 | return=-4323.63\n",
      "Episode 31 | steps=1488 | return=-4285.77\n",
      "Episode 32 | steps=1488 | return=-4287.78\n",
      "Episode 33 | steps=1488 | return=-4420.80\n",
      "Episode 34 | steps=1488 | return=-4290.88\n",
      "Episode 35 | steps=1488 | return=-4353.29\n",
      "Episode 36 | steps=1488 | return=-4313.71\n",
      "Episode 37 | steps=1488 | return=-4360.99\n",
      "Episode 38 | steps=1488 | return=-4312.52\n",
      "Episode 39 | steps=1488 | return=-4190.57\n",
      "Episode 40 | steps=1488 | return=-4318.56\n",
      "Episode 41 | steps=1488 | return=-4287.52\n",
      "Episode 42 | steps=1488 | return=-4244.53\n",
      "Episode 43 | steps=1488 | return=-4304.92\n",
      "Episode 44 | steps=1488 | return=-4311.00\n",
      "Episode 45 | steps=1488 | return=-4205.43\n",
      "Episode 46 | steps=1488 | return=-4281.59\n",
      "Episode 47 | steps=1488 | return=-4321.91\n",
      "Episode 48 | steps=1488 | return=-4160.57\n",
      "Episode 49 | steps=1488 | return=-4464.53\n",
      "Episode 50 | steps=1488 | return=-4363.15\n",
      "Episode 51 | steps=1488 | return=-4233.14\n",
      "Episode 52 | steps=1488 | return=-4272.90\n",
      "Episode 53 | steps=1488 | return=-4238.22\n",
      "Episode 54 | steps=1488 | return=-4326.43\n",
      "Episode 55 | steps=1488 | return=-4184.57\n",
      "Episode 56 | steps=1488 | return=-4246.02\n",
      "Episode 57 | steps=1488 | return=-4217.19\n",
      "Episode 58 | steps=1488 | return=-4170.38\n",
      "Episode 59 | steps=1488 | return=-4150.56\n",
      "Episode 60 | steps=1488 | return=-4225.66\n",
      "Episode 61 | steps=1488 | return=-4241.27\n",
      "Episode 62 | steps=1488 | return=-4270.98\n",
      "Episode 63 | steps=1488 | return=-4232.17\n",
      "Episode 64 | steps=1488 | return=-4188.73\n",
      "Episode 65 | steps=1488 | return=-4214.62\n",
      "Episode 66 | steps=1488 | return=-4235.02\n",
      "Episode 67 | steps=1488 | return=-4257.61\n",
      "Episode 68 | steps=1488 | return=-4237.58\n",
      "Episode 69 | steps=1488 | return=-4163.53\n",
      "Episode 70 | steps=1488 | return=-4188.07\n",
      "Episode 71 | steps=1488 | return=-4327.44\n",
      "Episode 72 | steps=1488 | return=-4210.61\n",
      "Episode 73 | steps=1488 | return=-4286.81\n",
      "Episode 74 | steps=1488 | return=-4255.15\n",
      "Episode 75 | steps=1488 | return=-4288.12\n",
      "Episode 76 | steps=1488 | return=-4388.40\n",
      "Episode 77 | steps=1488 | return=-4204.10\n",
      "Episode 78 | steps=1488 | return=-4138.65\n",
      "Episode 79 | steps=1488 | return=-4176.21\n",
      "Episode 80 | steps=1488 | return=-4337.54\n",
      "Episode 81 | steps=1488 | return=-4287.01\n",
      "Episode 82 | steps=1488 | return=-4228.12\n",
      "Episode 83 | steps=1488 | return=-4232.31\n",
      "Episode 84 | steps=1488 | return=-4338.94\n",
      "Episode 85 | steps=1488 | return=-4223.50\n",
      "Episode 86 | steps=1488 | return=-4358.48\n",
      "Episode 87 | steps=1488 | return=-4146.62\n",
      "Episode 88 | steps=1488 | return=-4248.20\n",
      "Episode 89 | steps=1488 | return=-4146.78\n",
      "Episode 90 | steps=1488 | return=-4251.15\n",
      "Episode 91 | steps=1488 | return=-4216.06\n",
      "Episode 92 | steps=1488 | return=-4185.72\n",
      "Episode 93 | steps=1488 | return=-4268.22\n",
      "Episode 94 | steps=1488 | return=-4344.07\n",
      "Episode 95 | steps=1488 | return=-4224.74\n",
      "Episode 96 | steps=1488 | return=-4201.20\n",
      "Episode 97 | steps=1488 | return=-4146.76\n",
      "Episode 98 | steps=1488 | return=-4168.88\n",
      "Episode 99 | steps=1488 | return=-4329.60\n",
      "Episode 100 | steps=1488 | return=-4296.05\n",
      "Episode 101 | steps=1488 | return=-4186.94\n",
      "Episode 102 | steps=1488 | return=-4266.25\n",
      "Episode 103 | steps=1488 | return=-4268.34\n",
      "Episode 104 | steps=1488 | return=-4286.21\n",
      "Episode 105 | steps=1488 | return=-4278.54\n",
      "Episode 106 | steps=1488 | return=-4146.64\n",
      "Episode 107 | steps=1488 | return=-4129.40\n",
      "Episode 108 | steps=1488 | return=-4243.69\n",
      "Episode 109 | steps=1488 | return=-4363.32\n",
      "Episode 110 | steps=1488 | return=-4242.77\n",
      "Episode 111 | steps=1488 | return=-4365.59\n",
      "Episode 112 | steps=1488 | return=-4282.43\n",
      "Episode 113 | steps=1488 | return=-4231.21\n",
      "Episode 114 | steps=1488 | return=-4249.33\n",
      "Episode 115 | steps=1488 | return=-4303.34\n",
      "Episode 116 | steps=1488 | return=-4156.47\n",
      "Episode 117 | steps=1488 | return=-4227.15\n",
      "Episode 118 | steps=1488 | return=-4263.34\n",
      "Episode 119 | steps=1488 | return=-4282.07\n",
      "Episode 120 | steps=1488 | return=-4219.76\n",
      "Episode 121 | steps=1488 | return=-4218.35\n",
      "Episode 122 | steps=1488 | return=-4250.58\n",
      "Episode 123 | steps=1488 | return=-4309.49\n",
      "Episode 124 | steps=1488 | return=-4196.41\n",
      "Episode 125 | steps=1488 | return=-4297.60\n",
      "Episode 126 | steps=1488 | return=-4239.45\n",
      "Episode 127 | steps=1488 | return=-4181.94\n",
      "Episode 128 | steps=1488 | return=-4189.87\n",
      "Episode 129 | steps=1488 | return=-4124.98\n",
      "Episode 130 | steps=1488 | return=-4292.70\n",
      "Episode 131 | steps=1488 | return=-4229.33\n",
      "Episode 132 | steps=1488 | return=-4330.66\n",
      "Episode 133 | steps=1488 | return=-4153.57\n",
      "Episode 134 | steps=1488 | return=-4214.78\n",
      "Episode 135 | steps=1488 | return=-4210.02\n",
      "Episode 136 | steps=1488 | return=-4250.55\n",
      "Episode 137 | steps=1488 | return=-4252.80\n",
      "Episode 138 | steps=1488 | return=-4325.64\n",
      "Episode 139 | steps=1488 | return=-4221.30\n",
      "Episode 140 | steps=1488 | return=-4251.87\n",
      "Episode 141 | steps=1488 | return=-4186.00\n",
      "Episode 142 | steps=1488 | return=-4164.85\n",
      "Episode 143 | steps=1488 | return=-4254.24\n",
      "Episode 144 | steps=1488 | return=-4202.12\n",
      "Episode 145 | steps=1488 | return=-4193.00\n",
      "Episode 146 | steps=1488 | return=-4179.29\n",
      "Episode 147 | steps=1488 | return=-4175.57\n",
      "Episode 148 | steps=1488 | return=-4252.31\n",
      "Episode 149 | steps=1488 | return=-4225.97\n",
      "Episode 150 | steps=1488 | return=-4300.50\n",
      "Episode 151 | steps=1488 | return=-4242.54\n",
      "Episode 152 | steps=1488 | return=-4165.42\n",
      "Episode 153 | steps=1488 | return=-4248.97\n",
      "Episode 154 | steps=1488 | return=-4221.89\n",
      "Episode 155 | steps=1488 | return=-4251.67\n",
      "Episode 156 | steps=1488 | return=-4265.87\n",
      "Episode 157 | steps=1488 | return=-4173.82\n",
      "Episode 158 | steps=1488 | return=-4246.55\n",
      "Episode 159 | steps=1488 | return=-4219.85\n",
      "Episode 160 | steps=1488 | return=-4276.22\n",
      "Episode 161 | steps=1488 | return=-4121.91\n",
      "Episode 162 | steps=1488 | return=-4299.70\n",
      "Episode 163 | steps=1488 | return=-4230.46\n",
      "Episode 164 | steps=1488 | return=-4347.04\n",
      "Episode 165 | steps=1488 | return=-4251.34\n",
      "Episode 166 | steps=1488 | return=-4244.31\n",
      "Episode 167 | steps=1488 | return=-4157.96\n",
      "Episode 168 | steps=1488 | return=-4214.44\n",
      "Episode 169 | steps=1488 | return=-4162.77\n",
      "Episode 170 | steps=1488 | return=-4179.54\n",
      "Episode 171 | steps=1488 | return=-4237.87\n",
      "Episode 172 | steps=1488 | return=-4159.98\n",
      "Episode 173 | steps=1488 | return=-4210.00\n",
      "Episode 174 | steps=1488 | return=-4221.22\n",
      "Episode 175 | steps=1488 | return=-4193.37\n",
      "Episode 176 | steps=1488 | return=-4254.74\n",
      "Episode 177 | steps=1488 | return=-4215.47\n",
      "Episode 178 | steps=1488 | return=-4243.46\n",
      "Episode 179 | steps=1488 | return=-4220.44\n",
      "Episode 180 | steps=1488 | return=-4222.79\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 75\u001b[0m\n\u001b[0;32m     72\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(np\u001b[38;5;241m.\u001b[39margmax(Q[s]))      \u001b[38;5;66;03m# exploit\u001b[39;00m\n\u001b[0;32m     74\u001b[0m next_obs, r, terminated, truncated, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(a)\n\u001b[1;32m---> 75\u001b[0m s_next \u001b[38;5;241m=\u001b[39m \u001b[43mdiscretize_safe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_obs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m best_next \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(Q[s_next])\n\u001b[0;32m     78\u001b[0m td_target \u001b[38;5;241m=\u001b[39m r \u001b[38;5;241m+\u001b[39m gamma \u001b[38;5;241m*\u001b[39m best_next\n",
      "Cell \u001b[1;32mIn[6], line 36\u001b[0m, in \u001b[0;36mdiscretize_safe\u001b[1;34m(obs_vec, info)\u001b[0m\n\u001b[0;32m     34\u001b[0m idxs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m v, edges \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(vals, BIN_EDGES):\n\u001b[1;32m---> 36\u001b[0m     i \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdigitize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medges\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[0;32m     37\u001b[0m     i \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mmin\u001b[39m(i, \u001b[38;5;28mlen\u001b[39m(edges)))\n\u001b[0;32m     38\u001b[0m     idxs\u001b[38;5;241m.\u001b[39mappend(i)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\ochre-gym-env\\lib\\site-packages\\numpy\\lib\\function_base.py:5733\u001b[0m, in \u001b[0;36mdigitize\u001b[1;34m(x, bins, right)\u001b[0m\n\u001b[0;32m   5731\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(bins) \u001b[38;5;241m-\u001b[39m _nx\u001b[38;5;241m.\u001b[39msearchsorted(bins[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], x, side\u001b[38;5;241m=\u001b[39mside)\n\u001b[0;32m   5732\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 5733\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_nx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearchsorted\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbins\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mside\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mside\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\ochre-gym-env\\lib\\site-packages\\numpy\\core\\fromnumeric.py:1400\u001b[0m, in \u001b[0;36msearchsorted\u001b[1;34m(a, v, side, sorter)\u001b[0m\n\u001b[0;32m   1332\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_searchsorted_dispatcher)\n\u001b[0;32m   1333\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msearchsorted\u001b[39m(a, v, side\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m, sorter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1334\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1335\u001b[0m \u001b[38;5;124;03m    Find indices where elements should be inserted to maintain order.\u001b[39;00m\n\u001b[0;32m   1336\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1398\u001b[0m \n\u001b[0;32m   1399\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1400\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msearchsorted\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mside\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mside\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msorter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msorter\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\ochre-gym-env\\lib\\site-packages\\numpy\\core\\fromnumeric.py:59\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bound(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------------------------- #\n",
    "# Helper: get hour from info\n",
    "# ---------------------------- #\n",
    "def get_hour_from_info(info):\n",
    "    \"\"\"Extracts hour of day from env info.\"\"\"\n",
    "    dt = info.get(\"Datetime\", None)\n",
    "    if dt is None:\n",
    "        return 0.0\n",
    "    if isinstance(dt, str):\n",
    "        dt = pd.to_datetime(dt)\n",
    "    return float(dt.hour)\n",
    "\n",
    "# ---------------------------- #\n",
    "# Safe discretizer (handles 3 or 4 features)\n",
    "# ---------------------------- #\n",
    "def discretize_safe(obs_vec, info=None):\n",
    "    \"\"\"Discretizes continuous observations to bins. Adds Hour if missing.\"\"\"\n",
    "    obs_vec = np.asarray(obs_vec).ravel()\n",
    "\n",
    "    # If \"Hour of Day\" is missing from obs, derive it from info\n",
    "    if obs_vec.shape[0] == 3:\n",
    "        hour = 0.0 if info is None else get_hour_from_info(info)\n",
    "        obs_vec = np.concatenate([obs_vec, [hour]])\n",
    "    elif obs_vec.shape[0] != 4:\n",
    "        raise ValueError(f\"Expected 3 or 4 features, got shape {obs_vec.shape}\")\n",
    "\n",
    "    tin, tout, price, hour = obs_vec\n",
    "    vals = [tin, tout, price, hour]\n",
    "    idxs = []\n",
    "    for v, edges in zip(vals, BIN_EDGES):\n",
    "        i = int(np.digitize(v, edges, right=False))\n",
    "        i = max(0, min(i, len(edges)))\n",
    "        idxs.append(i)\n",
    "    return tuple(idxs)\n",
    "\n",
    "# ---------------------------- #\n",
    "# Q-learning Hyperparameters\n",
    "# ---------------------------- #\n",
    "alpha = 0.2\n",
    "gamma = 0.99\n",
    "eps_start, eps_end, eps_decay = 1.0, 0.05, 20_000\n",
    "episodes = 500\n",
    "max_steps = 2000\n",
    "\n",
    "Q = defaultdict(lambda: np.zeros(A, dtype=np.float32))\n",
    "\n",
    "def epsilon(step):\n",
    "    frac = max(0.0, 1.0 - step / eps_decay)\n",
    "    return eps_end + (eps_start - eps_end) * frac\n",
    "\n",
    "global_step = 0\n",
    "ep_returns = []  # <--- store returns for plotting\n",
    "\n",
    "# ---------------------------- #\n",
    "# Q-learning training loop\n",
    "# ---------------------------- #\n",
    "for ep in range(episodes):\n",
    "    obs, info = env.reset()\n",
    "    s = discretize_safe(obs, info)\n",
    "    ep_return = 0.0\n",
    "\n",
    "    for t in range(max_steps):\n",
    "        e = epsilon(global_step)\n",
    "        if np.random.rand() < e:\n",
    "            a = np.random.randint(A)      # explore\n",
    "        else:\n",
    "            a = int(np.argmax(Q[s]))      # exploit\n",
    "\n",
    "        next_obs, r, terminated, truncated, info = env.step(a)\n",
    "        s_next = discretize_safe(next_obs, info)\n",
    "\n",
    "        best_next = np.max(Q[s_next])\n",
    "        td_target = r + gamma * best_next\n",
    "        td_error  = td_target - Q[s][a]\n",
    "        Q[s][a]  += alpha * td_error\n",
    "\n",
    "        s = s_next\n",
    "        ep_return += r\n",
    "        global_step += 1\n",
    "\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "    ep_returns.append(ep_return)\n",
    "    print(f\"Episode {ep+1:02d} | steps={t+1} | return={ep_return:.2f}\")\n",
    "\n",
    "# ---------------------------- #\n",
    "# Plot reward evolution\n",
    "# ---------------------------- #\n",
    "def moving_avg(x, k=5):\n",
    "    if k <= 1:\n",
    "        return np.asarray(x, dtype=float)\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    w = np.ones(k) / k\n",
    "    return np.convolve(x, w, mode=\"valid\")\n",
    "\n",
    "episodes_idx = np.arange(1, len(ep_returns) + 1)\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.plot(episodes_idx, ep_returns, label=\"Episode Return\", alpha=0.7)\n",
    "ma = moving_avg(ep_returns, k=5)\n",
    "plt.plot(np.arange(5, len(ep_returns) + 1), ma, label=\"5-Episode Moving Avg\", linewidth=2)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Return\")\n",
    "plt.title(\"Reward Evolution During Training\")\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e034591-747e-44c1-91df-286a51c8681d",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, info = env.reset()\n",
    "s = discretize_safe(obs, info)   # <-- use the safe version\n",
    "total = 0.0\n",
    "\n",
    "for t in range(2000):\n",
    "    a = int(np.argmax(Q[s]))     # greedy action (ε = 0)\n",
    "    obs, r, terminated, truncated, info = env.step(a)\n",
    "    s = discretize_safe(obs, info)  # <-- safe again\n",
    "    total += r\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "\n",
    "print(\"Evaluation return:\", total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c15ca4-8cdd-4373-8394-d37a63068bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_action_index_for_setpoint(target_sp, setpoints):\n",
    "    \"\"\"Find the discrete action index closest to a given setpoint value.\"\"\"\n",
    "    return int(np.argmin(np.abs(setpoints - target_sp)))\n",
    "\n",
    "def evaluate_policy(env, Q, episodes=1, greedy=True):\n",
    "    \"\"\"Evaluate either the trained RL policy (greedy) or baseline control.\"\"\"\n",
    "    totals = []\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        obs, info = env.reset()\n",
    "        s = discretize_safe(obs, info)\n",
    "        total = 0.0\n",
    "\n",
    "        for t in range(2000):\n",
    "            if greedy:\n",
    "                a = int(np.argmax(Q[s]))  # greedy (best learned action)\n",
    "            else:\n",
    "                # baseline: fixed action (e.g., 21°C)\n",
    "                baseline_idx = get_action_index_for_setpoint(21, SETPOINTS)\n",
    "                a = baseline_idx\n",
    "\n",
    "            obs, r, terminated, truncated, info = env.step(a)\n",
    "            s = discretize_safe(obs, info)\n",
    "            total += r\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "        totals.append(total)\n",
    "        print(f\"Episode {ep+1:02d} | return={total:.2f}\")\n",
    "\n",
    "    return np.mean(totals)\n",
    "\n",
    "# --- Run both evaluations ---\n",
    "rl_return = evaluate_policy(env, Q, episodes=3, greedy=True)\n",
    "baseline_return = evaluate_policy(env, Q, episodes=3, greedy=False)\n",
    "\n",
    "print(\"\\nAverage returns over 3 eval episodes:\")\n",
    "print(f\"RL Policy:     {rl_return:.2f}\")\n",
    "print(f\"Baseline (21°C): {baseline_return:.2f}\")\n",
    "print(f\"Improvement:   {rl_return - baseline_return:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebec3b60-9a36-41ee-a8fa-0ec2243f1065",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ===========================\n",
    "# Helpers\n",
    "# ===========================\n",
    "def safe_datetime(info, t, start=\"2018-01-01\", minutes=30):\n",
    "    dt = info.get(\"Datetime\", None) if isinstance(info, dict) else None\n",
    "    if dt is None:\n",
    "        return pd.Timestamp(start) + pd.Timedelta(minutes=minutes * t)\n",
    "    if isinstance(dt, str):\n",
    "        dt = pd.to_datetime(dt)\n",
    "    return dt\n",
    "\n",
    "def extract_indoor_temp(obs, info):\n",
    "    if isinstance(info, dict) and (\"Temperature - Indoor (C)\" in info):\n",
    "        return float(info[\"Temperature - Indoor (C)\"])\n",
    "    obs = np.asarray(obs).ravel()\n",
    "    return float(obs[0])\n",
    "\n",
    "def extract_outdoor_temp(obs, info):\n",
    "    \"\"\"Prefer outdoor temp from info; otherwise fall back to obs[1].\"\"\"\n",
    "    if isinstance(info, dict) and (\"Temperature - Outdoor (C)\" in info):\n",
    "        return float(info[\"Temperature - Outdoor (C)\"])\n",
    "    obs = np.asarray(obs).ravel()\n",
    "    return float(obs[1]) if obs.size >= 2 else np.nan\n",
    "\n",
    "def extract_obs_components(obs):\n",
    "    \"\"\"Assumes override order: [Tin, Tout, Price, Hour]. Falls back gracefully.\"\"\"\n",
    "    obs = np.asarray(obs).ravel()\n",
    "    tin  = float(obs[0]) if obs.size >= 1 else np.nan\n",
    "    tout = float(obs[1]) if obs.size >= 2 else np.nan\n",
    "    price= float(obs[2]) if obs.size >= 3 else np.nan\n",
    "    hour = float(obs[3]) if obs.size >= 4 else np.nan\n",
    "    return tin, tout, price, hour\n",
    "\n",
    "def get_comfort_band(env, default=(20.0, 23.0)):\n",
    "    for attr in (\"thermal_comfort_band\", \"comfort_band\", \"comfort_range\"):\n",
    "        if hasattr(env, attr):\n",
    "            band = getattr(env, attr)\n",
    "            if isinstance(band, (list, tuple)) and len(band) == 2:\n",
    "                return float(band[0]), float(band[1])\n",
    "    inner = getattr(env, \"env\", None)\n",
    "    if inner is not None:\n",
    "        for attr in (\"thermal_comfort_band\", \"comfort_band\", \"comfort_range\"):\n",
    "            if hasattr(inner, attr):\n",
    "                band = getattr(inner, attr)\n",
    "                if isinstance(band, (list, tuple)) and len(band) == 2:\n",
    "                    return float(band[0]), float(band[1])\n",
    "    return default\n",
    "\n",
    "# ===========================\n",
    "# Evaluations with action logging\n",
    "# ===========================\n",
    "def evaluate_policy_with_actions(env, Q, SETPOINTS, max_steps=2000):\n",
    "    data = []\n",
    "    obs, info = env.reset()\n",
    "    s = discretize_safe(obs, info)\n",
    "    for t in range(max_steps):\n",
    "        a = int(np.argmax(Q[s]))  # greedy\n",
    "        obs, r, terminated, truncated, info = env.step(a)\n",
    "        s = discretize_safe(obs, info)\n",
    "        tin, tout, price, hour = extract_obs_components(obs)\n",
    "        data.append({\n",
    "            \"Datetime\": safe_datetime(info, t),\n",
    "            \"ActionIdx_RL\": a,\n",
    "            \"Setpoint_RL\": float(SETPOINTS[a]),\n",
    "            \"IndoorTemp_RL\": extract_indoor_temp(obs, info),\n",
    "            \"OutdoorTemp_RL\": extract_outdoor_temp(obs, info),  # <-- added\n",
    "            \"Tout\": tout,\n",
    "            \"Price\": price,\n",
    "            \"Hour\": hour\n",
    "        })\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def evaluate_baseline(env, SETPOINTS, fixed_setpoint=21.5, max_steps=2000):\n",
    "    data = []\n",
    "    fixed_idx = int(np.argmin(np.abs(np.array(SETPOINTS) - fixed_setpoint)))\n",
    "    obs, info = env.reset()\n",
    "    for t in range(max_steps):\n",
    "        obs, r, terminated, truncated, info = env.step(fixed_idx)\n",
    "        tin, tout, price, hour = extract_obs_components(obs)\n",
    "        data.append({\n",
    "            \"Datetime\": safe_datetime(info, t),\n",
    "            \"ActionIdx_Base\": fixed_idx,\n",
    "            \"Setpoint_Baseline\": float(SETPOINTS[fixed_idx]),\n",
    "            \"IndoorTemp_Baseline\": extract_indoor_temp(obs, info),\n",
    "            \"OutdoorTemp_Base\": extract_outdoor_temp(obs, info),  # <-- added\n",
    "            \"Tout\": tout,\n",
    "            \"Price\": price,\n",
    "            \"Hour\": hour\n",
    "        })\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# ===========================\n",
    "# Run and align\n",
    "# ===========================\n",
    "rl_df   = evaluate_policy_with_actions(env, Q, SETPOINTS)\n",
    "# ---- ONLY CHANGE: set baseline to 21.0°C ----\n",
    "base_df = evaluate_baseline(env, SETPOINTS, fixed_setpoint=21.0)\n",
    "\n",
    "merged = pd.merge_asof(\n",
    "    rl_df.sort_values(\"Datetime\"),\n",
    "    base_df.sort_values(\"Datetime\"),\n",
    "    on=\"Datetime\",\n",
    "    direction=\"nearest\"\n",
    ")\n",
    "\n",
    "# Comfort band (variables from env)\n",
    "COMFORT_LOW, COMFORT_HIGH = get_comfort_band(env)\n",
    "\n",
    "# ===========================\n",
    "# Figure 1: Temps & Setpoints (styles per your spec)\n",
    "# ===========================\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.plot(merged[\"Datetime\"], merged[\"Setpoint_RL\"],        color=\"black\",  linestyle=\"-\",  linewidth=2, label=\"RL Setpoint (°C)\")\n",
    "plt.plot(merged[\"Datetime\"], merged[\"Setpoint_Baseline\"],  color=\"black\",  linestyle=\"--\", linewidth=2, label=\"Baseline Setpoint (°C)\")\n",
    "\n",
    "plt.plot(merged[\"Datetime\"], merged[\"IndoorTemp_RL\"],       color=\"magenta\", linestyle=\"-\",  alpha=0.9, label=\"RL Indoor Temp (°C)\")\n",
    "plt.plot(merged[\"Datetime\"], merged[\"IndoorTemp_Baseline\"], color=\"magenta\", linestyle=\"--\", alpha=0.9, label=\"Baseline Indoor Temp (°C)\")\n",
    "\n",
    "plt.hlines(COMFORT_LOW,  merged[\"Datetime\"].iloc[0], merged[\"Datetime\"].iloc[-1],\n",
    "           colors=\"blue\", linestyles=\":\", linewidth=2, label=f\"Lower Comfort ({COMFORT_LOW} °C)\")\n",
    "plt.hlines(COMFORT_HIGH, merged[\"Datetime\"].iloc[0], merged[\"Datetime\"].iloc[-1],\n",
    "           colors=\"red\",  linestyles=\":\", linewidth=2, label=f\"Upper Comfort ({COMFORT_HIGH} °C)\")\n",
    "\n",
    "plt.xlabel(\"Datetime\")\n",
    "plt.ylabel(\"Temperature (°C)\")\n",
    "plt.title(\"Indoor Temperature & Setpoint: RL (solid) vs Baseline (dashed)\")\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ===========================\n",
    "# Figure 2: RL action index over time + histogram\n",
    "# ===========================\n",
    "fig, ax = plt.subplots(2, 1, figsize=(10, 6), sharex=False)\n",
    "\n",
    "ax[0].step(rl_df[\"Datetime\"], rl_df[\"ActionIdx_RL\"], where=\"post\")\n",
    "ax[0].set_ylabel(\"RL Action Index\")\n",
    "ax[0].set_title(\"RL Action Index Over Time\")\n",
    "ax[0].grid(True, linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "counts = rl_df[\"ActionIdx_RL\"].value_counts().sort_index()\n",
    "ax[1].bar(counts.index.astype(int), counts.values)\n",
    "ax[1].set_xlabel(\"Action Index\")\n",
    "ax[1].set_ylabel(\"Count\")\n",
    "ax[1].set_title(\"RL Action Usage\")\n",
    "ax[1].grid(True, linestyle=\"--\", alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ===========================\n",
    "# Print mapping and counts\n",
    "# ===========================\n",
    "unique_actions = sorted(rl_df[\"ActionIdx_RL\"].unique().tolist())\n",
    "print(\"\\nAction index → setpoint mapping:\")\n",
    "for idx in unique_actions:\n",
    "    print(f\"  {idx} → {SETPOINTS[idx]:.1f} °C\")\n",
    "\n",
    "print(\"\\nAction counts:\")\n",
    "for idx in unique_actions:\n",
    "    print(f\"  index {idx} ({SETPOINTS[idx]:.1f} °C): {int((rl_df['ActionIdx_RL']==idx).sum())} steps\")\n",
    "\n",
    "# ===========================\n",
    "# Figure 3: Outdoor temperature evolution (corrected)\n",
    "# ===========================\n",
    "tout_series = merged.get(\"OutdoorTemp_RL\", rl_df.get(\"OutdoorTemp_RL\"))\n",
    "if tout_series is None or len(tout_series) == 0:\n",
    "    tout_series = merged.get(\"OutdoorTemp_Base\", base_df.get(\"OutdoorTemp_Base\"))\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(merged[\"Datetime\"], tout_series, linewidth=2, label=\"Outdoor Temp (°C)\")\n",
    "plt.xlabel(\"Datetime\")\n",
    "plt.ylabel(\"Temperature (°C)\")\n",
    "plt.title(\"Outdoor Temperature Evolution\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ===========================\n",
    "# Figure 4: TOU price evolution (step plot)\n",
    "# ===========================\n",
    "if \"Price_x\" in merged.columns:\n",
    "    price_series = merged[\"Price_x\"]\n",
    "elif \"Price\" in merged.columns:\n",
    "    price_series = merged[\"Price\"]\n",
    "else:\n",
    "    price_series = merged.get(\"Price_y\", pd.Series(index=merged.index, dtype=float))\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.step(merged[\"Datetime\"], price_series, where=\"post\", linewidth=2, label=\"TOU Price ($/kWh)\")\n",
    "plt.xlabel(\"Datetime\")\n",
    "plt.ylabel(\"Price ($/kWh)\")\n",
    "plt.title(\"TOU Price Evolution\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69d9788-3d07-4c9d-ad44-6a62fe864aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---- helpers ----\n",
    "def pick(df, *cands):\n",
    "    \"\"\"Return the first existing column among candidates; else raise.\"\"\"\n",
    "    for c in cands:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    raise KeyError(f\"None of {cands} found in dataframe columns: {list(df.columns)}\")\n",
    "\n",
    "def bin_series(values, edges):\n",
    "    # np.digitize returns 1..len(edges); clamp to [0, len(edges)]\n",
    "    i = np.digitize(values, edges, right=False).astype(int)\n",
    "    return np.clip(i, 0, len(edges))\n",
    "\n",
    "# ---- map your logged columns (IndoorTemp_RL etc.) to generic names ----\n",
    "TIN_COL    = pick(rl_df, \"Tin\", \"IndoorTemp_RL\", \"IndoorTemp\")         # <- your data has \"IndoorTemp_RL\"\n",
    "TOUT_COL   = pick(rl_df, \"Tout\", \"OutdoorTemp_RL\", \"OutdoorTemp\")\n",
    "PRICE_COL  = pick(rl_df, \"Price\", \"Price_RL\")\n",
    "HOUR_COL   = pick(rl_df, \"Hour\")\n",
    "ACT_COL    = pick(rl_df, \"ActionIdx_RL\", \"Action\")                     # action index chosen by RL\n",
    "SP_COL     = pick(rl_df, \"Setpoint_RL\", \"Setpoint\")\n",
    "TIME_COL   = pick(rl_df, \"Datetime\", \"time\", \"timestamp\")\n",
    "\n",
    "# ---- build a tidy frame with bins and features ----\n",
    "rl_bins = pd.DataFrame({\n",
    "    \"Datetime\": rl_df[TIME_COL].values,\n",
    "    \"Tin\":      rl_df[TIN_COL].astype(float).values,\n",
    "    \"Tout\":     rl_df[TOUT_COL].astype(float).values,\n",
    "    \"Price\":    rl_df[PRICE_COL].astype(float).values,\n",
    "    \"Hour\":     rl_df[HOUR_COL].astype(float).values,\n",
    "    \"Action\":   rl_df[ACT_COL].astype(int).values,\n",
    "    \"Setpoint\": rl_df[SP_COL].astype(float).values,\n",
    "})\n",
    "rl_bins[\"Tin_bin\"]   = bin_series(rl_bins[\"Tin\"].values,   BIN_EDGES[0])\n",
    "rl_bins[\"Tout_bin\"]  = bin_series(rl_bins[\"Tout\"].values,  BIN_EDGES[1])\n",
    "rl_bins[\"Price_bin\"] = bin_series(rl_bins[\"Price\"].values, BIN_EDGES[2])\n",
    "rl_bins[\"Hour_bin\"]  = bin_series(rl_bins[\"Hour\"].values,  BIN_EDGES[3])\n",
    "\n",
    "# ---- find action changes (the \"spikes\") ----\n",
    "changes = rl_bins[rl_bins[\"Action\"].diff().fillna(0) != 0].copy()\n",
    "prev = rl_bins.shift(1).loc[changes.index]\n",
    "changes[\"Datetime_prev\"] = prev[\"Datetime\"].values\n",
    "changes[\"Action_prev\"]   = prev[\"Action\"].values\n",
    "changes[\"Setpoint_prev\"] = prev[\"Setpoint\"].values\n",
    "\n",
    "for col in [\"Tin_bin\",\"Tout_bin\",\"Price_bin\",\"Hour_bin\"]:\n",
    "    changes[f\"{col}_changed\"] = (rl_bins[col].diff().fillna(0) != 0).loc[changes.index].values\n",
    "\n",
    "def which_changed(row):\n",
    "    out = []\n",
    "    if row[\"Tin_bin_changed\"]:   out.append(\"Tin_bin\")\n",
    "    if row[\"Tout_bin_changed\"]:  out.append(\"Tout_bin\")\n",
    "    if row[\"Price_bin_changed\"]: out.append(\"Price_bin\")\n",
    "    if row[\"Hour_bin_changed\"]:  out.append(\"Hour_bin\")\n",
    "    return out\n",
    "\n",
    "summary = []\n",
    "for _, r in changes.iterrows():\n",
    "    summary.append({\n",
    "        \"Datetime\": r[\"Datetime\"],\n",
    "        \"Setpoint prev→new\": f\"{r['Setpoint_prev']:.1f}→{r['Setpoint']:.1f}\",\n",
    "        \"Action prev→new\":   f\"{int(r['Action_prev'])}→{int(r['Action'])}\",\n",
    "        \"Tin (°C)\":          f\"{r['Tin']:.2f}\",\n",
    "        \"Tout (°C)\":         f\"{r['Tout']:.2f}\",\n",
    "        \"Price\":             f\"{r['Price']:.3f}\",\n",
    "        \"Hour\":              f\"{r['Hour']:.0f}\",\n",
    "        \"Bins changed\":      which_changed(r)\n",
    "    })\n",
    "\n",
    "spike_report = pd.DataFrame(summary)\n",
    "print(\"=== RL setpoint/action changes ===\")\n",
    "print(spike_report.to_string(index=False) if not spike_report.empty else \"No changes detected.\")\n",
    "\n",
    "# ---- zoom plot around the first spike ----\n",
    "if not spike_report.empty:\n",
    "    first_idx = changes.index[0]\n",
    "    N = 16  # window half-width in steps\n",
    "    lo = max(0, first_idx - N)\n",
    "    hi = min(len(rl_bins)-1, first_idx + N)\n",
    "    zoom = rl_bins.loc[lo:hi].copy()\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(10,4))\n",
    "    ax1.plot(zoom[\"Datetime\"], zoom[\"Setpoint\"], color=\"black\", label=\"RL Setpoint (°C)\", linewidth=2)\n",
    "    ax1.plot(zoom[\"Datetime\"], zoom[\"Tin\"], color=\"magenta\", alpha=0.9, label=\"RL Indoor Temp (°C)\")\n",
    "    ax1.set_xlabel(\"Datetime\"); ax1.set_ylabel(\"Temp (°C)\")\n",
    "    ax1.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.step(zoom[\"Datetime\"], zoom[\"Price\"], where=\"post\", alpha=0.7, label=\"TOU Price\")\n",
    "    ax2.set_ylabel(\"Price\")\n",
    "\n",
    "    ax1.axvline(rl_bins.loc[first_idx, \"Datetime\"], color=\"gray\", linestyle=\":\", linewidth=2)\n",
    "\n",
    "    l1, lab1 = ax1.get_legend_handles_labels()\n",
    "    l2, lab2 = ax2.get_legend_handles_labels()\n",
    "    ax1.legend(l1+l2, lab1+lab2, loc=\"upper left\")\n",
    "\n",
    "    plt.title(\"Zoom around RL setpoint change\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b0f940-5c20-4320-b17a-f37da670a72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from stable_baselines3 import DQN\n",
    "# model = DQN(\"MlpPolicy\", env, learning_rate=1e-4, buffer_size=100_000, verbose=1)\n",
    "# model.learn(total_timesteps=400_000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cad63d7-7e86-45de-9672-629e4778787d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ochre-gym-env)",
   "language": "python",
   "name": "ochre-gym-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
